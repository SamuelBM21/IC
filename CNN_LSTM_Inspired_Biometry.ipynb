{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelBM21/IC/blob/main/CNN_LSTM_Inspired_Biometry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-h55ManZSRfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953b01aa-064b-46a9-d5a2-b66083b5f0e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch                        #PyTorch\n",
        "import torch.nn as nn               #Camadas da Rede neural\n",
        "import torch.optim as optim         #Otimizadores\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "import seaborn as sns\n",
        "import copy\n",
        "import gc\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "YLxc8cGQSloP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2480d2-5d5d-4fe6-d12a-7c9c1a5adaa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "illZBHfKWHhI"
      },
      "outputs": [],
      "source": [
        "class EEGWindowDataset(Dataset):\n",
        "    def __init__(self, folder_path, subjects, tasks, sampling_points=1920, offset=35, label_map=None, transform=None):\n",
        "        self.windows = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "        for subj in subjects:\n",
        "            for task in tasks:\n",
        "                file_name = f\"S{subj:03d}{task}.csv\"\n",
        "                file_path = os.path.join(folder_path, f\"S{subj:03d}\", file_name)\n",
        "\n",
        "                if not os.path.exists(file_path):\n",
        "                    print(f\"[AVISO] Arquivo não encontrado: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                data = np.loadtxt(file_path, delimiter=',')  # (64, total_amostras)\n",
        "                total_points = data.shape[1]\n",
        "\n",
        "                for start in range(0, total_points - sampling_points + 1, offset):\n",
        "                    end = start + sampling_points\n",
        "                    window = data[:, start:end]\n",
        "                    if self.transform:\n",
        "                        window = self.transform(window)\n",
        "                    self.windows.append(window)\n",
        "                    self.labels.append(subjects.index(subj))  # índice no vetor de subjects\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.windows[idx], dtype=torch.float32).permute(1, 0) # Transpose to [1920, 64]\n",
        "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d723c545"
      },
      "outputs": [],
      "source": [
        "class EEGFixedTestDataset(Dataset):\n",
        "    def __init__(self, folder_path, subjects, tasks=['R01', 'R02'], sampling_points=1920, label_map=None):\n",
        "        self.windows = []\n",
        "        self.labels = []\n",
        "\n",
        "        for subj in subjects:\n",
        "            for task in tasks:\n",
        "                file_name = f\"S{subj:03d}{task}.csv\"\n",
        "                file_path = os.path.join(folder_path, f\"S{subj:03d}\", file_name)\n",
        "\n",
        "                if not os.path.exists(file_path):\n",
        "                    print(f\"[AVISO] Arquivo não encontrado: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                data = np.loadtxt(file_path, delimiter=',')  # shape: (64, total_amostras)\n",
        "                total_points = data.shape[1]\n",
        "\n",
        "                step = (total_points - sampling_points) // 4\n",
        "                for i in range(5):\n",
        "                    start = i * step\n",
        "                    end = start + sampling_points\n",
        "                    window = data[:, start:end]\n",
        "                    self.windows.append(window)\n",
        "                    self.labels.append(subjects.index(subj))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.windows)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.windows[idx], dtype=torch.float32).permute(1, 0) # Transpose to [1920, 64]\n",
        "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NFM1x_mWfmc"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "learning_rate = 0.01\n",
        "epochs = 40\n",
        "\n",
        "# Todos os sujeitos (1-109)\n",
        "all_subjects = list(range(1, 110))\n",
        "num_classes = len(all_subjects)  # 109 classes\n",
        "subject_names = [f\"Subject {i}\" for i in all_subjects]\n",
        "\n",
        "# Mapeamento de sujeito para label (0-108)\n",
        "subject_to_label = {subj: idx for idx, subj in enumerate(all_subjects)}\n",
        "\n",
        "#Carrega todo o dataset\n",
        "full_dataset = EEGWindowDataset(\n",
        "    folder_path='/content/drive/MyDrive/EEGDataset/Dataset_CSV',\n",
        "    subjects=all_subjects,\n",
        "    tasks=['R01'],\n",
        "    sampling_points=1920,\n",
        "    offset=35,\n",
        "    label_map=subject_to_label\n",
        ")\n",
        "\n",
        "#Extrai todos os índices\n",
        "indices = list(range(len(full_dataset)))\n",
        "labels = [full_dataset.labels[i] for i in indices]\n",
        "\n",
        "# Faz split estratificado (10% para validação)\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices,\n",
        "    test_size=0.1,\n",
        "    stratify=labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "train_dataset = Subset(full_dataset, train_idx)\n",
        "val_dataset = Subset(full_dataset, val_idx)\n",
        "\n",
        "test_dataset = EEGFixedTestDataset(\n",
        "    folder_path='/content/drive/MyDrive/EEGDataset/Dataset_CSV',\n",
        "    subjects=all_subjects,  # TODOS os sujeitos\n",
        "    tasks=['R02'],  # OLHOS FECHADOS (teste)\n",
        "    sampling_points=1920,\n",
        "    label_map=subject_to_label\n",
        ")\n",
        "\n",
        "\n",
        "# Nomes das classes para relatórios (109 sujeitos)\n",
        "class_names = [f\"Subject {i}\" for i in all_subjects]\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoP56nYsXa5w"
      },
      "outputs": [],
      "source": [
        "class EEG_BRNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_channels=64, time_steps=1920, hidden_size=128, num_layers=3, num_classes=109):\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN para extração espacial por canal\n",
        "        self.cnn = nn.Sequential(\n",
        "\n",
        "            #Conv1\n",
        "            nn.Conv1d(128, 96, kernel_size=11),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(96),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "\n",
        "            #Conv2\n",
        "            nn.Conv1d(96, 128, kernel_size=9),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "\n",
        "            #Conv3\n",
        "            nn.Conv1d(128, 256, kernel_size=9), # [B, 256, 227]\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.MaxPool1d(kernel_size=2),        # [B, 256, 113]\n",
        "        )\n",
        "\n",
        "        # LSTM bidirecional sobre sequência temporal\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_channels,  # output da CNN\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,      # Deveria ser 5\n",
        "            batch_first=True,\n",
        "        )\n",
        "\n",
        "        # Classificador\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 113, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4096, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, 1920, 64]\n",
        "        out, _ = self.lstm(x)            # [B, 1920, 128]\n",
        "        x = out.permute(0, 2, 1)         # [B, 128, 1920]\n",
        "        x = self.cnn(x)                  # [B, 256, 113]\n",
        "        x = self.classifier(x)           # [B, 109]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzQ6d0JfSsoa"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EEG_BRNN_LSTM(num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
        "    optimizer, milestones=[2, 37], gamma=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b04ohZheSsl2"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=100):\n",
        "    best_f1 = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    patience, counter = 10, 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validação\n",
        "        val_loss, val_f1 = evaluate_model(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Época {epoch+1}/{epochs} | Train Loss: {running_loss/len(train_loader):.4f} | Val Loss: {val_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_f1 > best_f1:\n",
        "            best_f1 = val_f1\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Carregar melhor modelo\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxcPAZj3yXMW"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_loader, device, class_names):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calcular métricas principais\n",
        "    accuracy = (np.array(all_preds) == np.array(all_labels)).mean()\n",
        "    print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Matriz de confusão reduzida (apenas erros)\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Calcular acurácia por sujeito\n",
        "    subject_acc = {}\n",
        "    for i in range(num_classes):\n",
        "        idx = np.where(np.array(all_labels) == i)[0]\n",
        "        if len(idx) > 0:\n",
        "            subject_acc[i] = (np.array(all_preds)[idx] == i).mean()\n",
        "\n",
        "    # Plotar acurácia por sujeito\n",
        "    plt.figure(figsize=(15, 6))\n",
        "    plt.bar(range(num_classes), [subject_acc.get(i, 0) for i in range(num_classes)])\n",
        "    plt.xlabel(\"Subject ID\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Accuracy per Subject\")\n",
        "    plt.xticks(range(num_classes), class_names, rotation=90)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Matriz de confusão para os 10 sujeitos com pior desempenho\n",
        "    worst_subjects = sorted(subject_acc, key=subject_acc.get)[:10]\n",
        "    worst_indices = [i for i in range(len(all_labels)) if all_labels[i] in worst_subjects]\n",
        "\n",
        "    if worst_indices:\n",
        "        cm_worst = confusion_matrix(\n",
        "            np.array(all_labels)[worst_indices],\n",
        "            np.array(all_preds)[worst_indices],\n",
        "            labels=worst_subjects\n",
        "        )\n",
        "\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        sns.heatmap(cm_worst, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=[class_names[i] for i in worst_subjects],\n",
        "                    yticklabels=[class_names[i] for i in worst_subjects])\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"True\")\n",
        "        plt.title(\"Confusion Matrix for Worst 10 Subjects\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWM2U3DSmCZS"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = running_loss / len(data_loader)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    return avg_loss, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0o42BeXpSsjH"
      },
      "outputs": [],
      "source": [
        "model = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JBwU3yGizI4"
      },
      "outputs": [],
      "source": [
        "print(\"\\nPerformance on Training Data (Eyes Open):\")\n",
        "test_model(model, train_loader, device, subject_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbV6zkABzG72"
      },
      "outputs": [],
      "source": [
        "print(\"\\nPerformance on Test Data (Eyes Closed):\")\n",
        "test_model(model, test_loader, device, subject_names)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}